{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Long-Term Memory from a Conversation\n",
    "This Notebook shows how to set up an agent that can extract memories from a conversation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install openai==1.12.0 langchain==0.1.6 langchain_openai==0.0.5\n",
    "#!pip install redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-\"\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"Meal-Planner\""
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Agent: Memory Sentinel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    ")\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "system_prompt_initial = \"\"\"\n",
    "Your job is to assess a brief chat history in order to determine if the conversation contains any details about a family's dining habits. \n",
    "\n",
    "You are part of a team building a knowledge base regarding a family's dining habits to assist in highly customized meal planning.\n",
    "\n",
    "You play the critical role of assessing the message to determine if it contains any information worth recording in the knowledge base.\n",
    "\n",
    "You are only interested in the following categories of information:\n",
    "\n",
    "1. The family's food allergies (e.g. a dairy or soy allergy)\n",
    "2. Foods the family likes (e.g. likes pasta)\n",
    "3. Foods the family dislikes (e.g. doesn't eat mussels)\n",
    "4. Attributes about the family that may impact weekly meal planning (e.g. lives in Austin; has a husband and 2 children; has a garden; likes big lunches; etc.)\n",
    "\n",
    "When you receive a message, you perform a sequence of steps consisting of:\n",
    "\n",
    "1. Analyze the message for information.\n",
    "2. If it has any information worth recording, return TRUE. If not, return FALSE.\n",
    "\n",
    "You should ONLY RESPOND WITH TRUE OR FALSE. Absolutely no other information should be provided.\n",
    "\n",
    "Take a deep breath, think step by step, and then analyze the following message:\n",
    "\"\"\"\n",
    "\n",
    "# Get the prompt to use - you can modify this!\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessagePromptTemplate.from_template(system_prompt_initial),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Remember, only respond with TRUE or FALSE. Do not provide any other information.\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Choose the LLM that will drive the agent\n",
    "llm = ChatOpenAI(\n",
    "    #model=\"gpt-4-0125-preview\",\n",
    "    model=\"gpt-3.5-turbo-0125\",\n",
    "    streaming=True,\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "sentinel_runnable = {\"messages\": RunnablePassthrough()} | prompt | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Meal Planner Agent"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "system_prompt_initial = \"\"\"Du erstellst immer ein Gericht wenn du dazu aufgefordert wirst. Das Gericht basiert auf dem nachfolgenden Wissen: {memories}\n",
    "Wenn du nicht dazu aufgefordert wirst, dann weise darauf hin, dass du nur Gerichte machen kannst.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessagePromptTemplate.from_template(system_prompt_initial),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Make it tasty\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    #model=\"gpt-4-0125-preview\",\n",
    "    model=\"gpt-3.5-turbo-0125\",\n",
    "    streaming=True,\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "meal_planner_runnable = prompt | llm"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Agent: Memory Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import redis\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from langchain.tools import StructuredTool\n",
    "from enum import Enum\n",
    "from typing import Optional, Annotated\n",
    "\n",
    "# Redis-Verbindung herstellen\n",
    "redis_client = redis.Redis(host='localhost', port=6379, db=0, decode_responses=True)\n",
    "\n",
    "class Category(str, Enum):\n",
    "    Food_Allergy = \"Allergy\"\n",
    "    Food_Like = \"Like\"\n",
    "    Food_Dislike = \"Dislike\"\n",
    "    Family_Attribute = \"Attribute\"\n",
    "\n",
    "\n",
    "class Action(str, Enum):\n",
    "    Create = \"Create\"\n",
    "    Update = \"Update\"\n",
    "    Delete = \"Delete\"\n",
    "\n",
    "\n",
    "class AddKnowledge(BaseModel):\n",
    "    knowledge: str = Field(\n",
    "        ...,\n",
    "        description=\"Condensed bit of knowledge to be saved for future reference in the format: [person(s) this is relevant to] [fact to store] (e.g. Husband doesn't like tuna; I am allergic to shellfish; etc)\",\n",
    "    )\n",
    "    knowledge_old: Optional[str] = Field(\n",
    "        None,\n",
    "        description=\"If updating or deleting record, the complete, exact phrase that needs to be modified\",\n",
    "    )\n",
    "    category: Category = Field(\n",
    "        ..., description=\"Category that this knowledge belongs to\"\n",
    "    )\n",
    "    action: Action = Field(\n",
    "        ...,\n",
    "        description=\"Whether this knowledge is adding a new record, updating a record, or deleting a record\",\n",
    "    )\n",
    "\n",
    "\n",
    "def get_knowledge() -> list:\n",
    "    \"\"\"\n",
    "    Gibt den gesamten Inhalt der Redis-Datenbank zurück.\n",
    "    Jeder Eintrag wird als Dictionary mit Kategorie, Wissen und Wert zurückgegeben.\n",
    "    \"\"\"\n",
    "    all_keys = redis_client.keys('*')\n",
    "    all_entries = []\n",
    "    for key in all_keys:\n",
    "        value = redis_client.get(key)\n",
    "        category, knowledge = key.split(':', 1)\n",
    "        entry = {\"category\": category, \"knowledge\": knowledge, \"value\": value}\n",
    "        all_entries.append(entry)\n",
    "    return all_entries\n",
    "\n",
    "def modify_knowledge(\n",
    "        knowledge: str,\n",
    "        category: str,\n",
    "        action: str,\n",
    "        knowledge_old: str = \"\",\n",
    ") -> dict:\n",
    "    if action == \"Get\":\n",
    "        # Ruft alle Einträge aus der Redis-Datenbank ab\n",
    "        return {\"result\": \"Retrieved Knowledge\", \"data\": get_knowledge()}\n",
    "    elif action == \"Create\":\n",
    "        redis_key = f\"{category}:{knowledge}\"\n",
    "        if redis_client.exists(redis_key):\n",
    "            return {\"error\": \"Knowledge already exists\"}\n",
    "        redis_client.set(redis_key, knowledge)\n",
    "        print(\"Creating Knowledge: \", knowledge, category)\n",
    "        return {\"result\": \"Created\"}\n",
    "    elif action == \"Update\":\n",
    "        if knowledge_old:\n",
    "            redis_key_old = f\"{category}:{knowledge_old}\"\n",
    "            redis_key_new = f\"{category}:{knowledge}\"\n",
    "            if redis_client.exists(redis_key_old):\n",
    "                redis_client.rename(redis_key_old, redis_key_new)\n",
    "                redis_client.set(redis_key_new, knowledge)\n",
    "                print(\"Updating Knowledge: \", knowledge_old, \"to\", knowledge, category)\n",
    "                return {\"result\": \"Updated\"}\n",
    "            else:\n",
    "                return {\"error\": \"Old knowledge not found\"}\n",
    "        \n",
    "        else:\n",
    "            return {\"error\": \"Old knowledge required for update\"}\n",
    "    elif action == \"Delete\":\n",
    "        redis_key = f\"{category}:{knowledge}\"\n",
    "        if redis_client.exists(redis_key):\n",
    "            redis_client.delete(redis_key)\n",
    "            print(\"Deleting Knowledge: \", knowledge, category)\n",
    "            return {\"result\": \"Deleted\"}\n",
    "        else:\n",
    "            return {\"error\": \"Knowledge not found to delete\"}\n",
    "    else:\n",
    "        return {\"error\": \"Invalid action\"}\n",
    "\n",
    "\n",
    "# Angenommen, AddKnowledge ist bereits definiert und wird als Input-Schema verwendet\n",
    "tool_modify_knowledge = StructuredTool.from_function(\n",
    "    func=modify_knowledge,\n",
    "    name=\"Knowledge_Modifier\",\n",
    "    description=\"Add, update, or delete a bit of knowledge\",\n",
    "    args_schema=AddKnowledge,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the tools to execute them from the graph\n",
    "from langgraph.prebuilt import ToolExecutor\n",
    "\n",
    "# Set up the agent's tools\n",
    "agent_tools = [tool_modify_knowledge]\n",
    "\n",
    "tool_executor = ToolExecutor(agent_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    ")\n",
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "\n",
    "# die knowledge experts sind die tools\n",
    "\n",
    "system_prompt_initial = \"\"\"\n",
    "You are a supervisor managing a team of knowledge experts.\n",
    "\n",
    "Your team's job is to create a perfect knowledge base about a family's dining habits to assist in highly customized meal planning.\n",
    "\n",
    "The knowledge base should ultimately consist of many discrete pieces of information that add up to a rich persona (e.g. I like pasta; I am allergic to shellfish; I don't eat mussels; I live in Austin, Texas; I have a husband and 2 children aged 5 and 7).\n",
    "\n",
    "Every time you receive a message, you will evaluate if it has any information worth recording in the knowledge base.\n",
    "IGNORE EVERYTHING ELSE! But extract the relevant information!\n",
    "\n",
    "A message may contain multiple pieces of information that should be saved separately.\n",
    "\n",
    "You are only interested in the following categories of information:\n",
    "\n",
    "1. The family's food allergies (e.g. a dairy or soy allergy) - These are important to know because they can be life-threatening. Only log something as an allergy if you are certain it is an allergy and not just a dislike.\n",
    "2. Foods the family likes (e.g. likes pasta) - These are important to know because they can help you plan meals, but are not life-threatening.\n",
    "3. Foods the family dislikes (e.g. doesn't eat mussels or rarely eats beef) - These are important to know because they can help you plan meals, but are not life-threatening.\n",
    "4. Attributes about the family that may impact weekly meal planning (e.g. lives in Austin; has a husband and 2 children; has a garden; likes big lunches, etc.)\n",
    "\n",
    "When you receive a message, you perform a sequence of steps consisting of:\n",
    "\n",
    "1. Analyze the most recent Human message for information. You will see multiple messages for context, but we are only looking for new information in the most recent message.\n",
    "2. Compare this to the knowledge you already have.\n",
    "3. Determine if this is new knowledge, an update to old knowledge that now needs to change, or should result in deleting information that is not correct. It's possible that a food you previously wrote as a dislike might now be a like, or that a family member who previously liked a food now dislikes it - those examples would require an update.\n",
    "\n",
    "Here are the existing bits of information that we have about the family.\n",
    "\n",
    "```\n",
    "{memories}\n",
    "```\n",
    "\n",
    "Call the right tools to save the information, then respond with DONE. If you identiy multiple pieces of information, call everything at once. You only have one chance to call tools.\n",
    "\n",
    "I will tip you $20 if you are perfect, and I will fine you $40 if you miss any important information or change any incorrect information.\n",
    "\n",
    "Take a deep breath, think step by step, and then analyze the following message:\n",
    "\"\"\"\n",
    "\n",
    "# Get the prompt to use - you can modify this!\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessagePromptTemplate.from_template(system_prompt_initial),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Choose the LLM that will drive the agent\n",
    "llm = ChatOpenAI(\n",
    "    # model=\"gpt-3.5-turbo-0125\",\n",
    "    model=\"gpt-4-0125-preview\",\n",
    "    streaming=True,\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "# Create the tools to bind to the model\n",
    "tools = [convert_to_openai_function(t) for t in agent_tools]\n",
    "\n",
    "knowledge_master_runnable = prompt | llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Sequence\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "\n",
    "    #messages: Sequence[BaseMessage]\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "\n",
    "    memories: Sequence[str]\n",
    "\n",
    "    contains_information: str"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain_core.messages import ToolMessage\n",
    "from langgraph.prebuilt import ToolInvocation\n",
    "\n",
    "\n",
    "def call_sentinel(state):\n",
    "    messages = state[\"messages\"]\n",
    "    response = sentinel_runnable.invoke(messages)\n",
    "    return {\"contains_information\": \"TRUE\" in response.content and \"yes\" or \"no\"}\n",
    "\n",
    "\n",
    "def call_meal_planner(state):\n",
    "    messages = state[\"messages\"]\n",
    "    memories = state[\"memories\"]\n",
    "    response = meal_planner_runnable.invoke(\n",
    "        {\"messages\": messages, \"memories\": memories}\n",
    "    )\n",
    "    return {\"messages\": response}\n",
    "\n",
    "# Define the function that determines whether to continue or not\n",
    "def should_continue(state):\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    # If there are no tool calls, then we finish\n",
    "    if \"tool_calls\" not in last_message.additional_kwargs:\n",
    "        return \"end\"\n",
    "    # Otherwise, we continue\n",
    "    else:\n",
    "        return \"continue\"\n",
    "\n",
    "\n",
    "# Define the function that calls the knowledge master\n",
    "def call_knowledge_master(state):\n",
    "    messages = state[\"messages\"]\n",
    "    memories = state[\"memories\"]\n",
    "    response = knowledge_master_runnable.invoke(\n",
    "        {\"messages\": messages, \"memories\": memories}\n",
    "    )\n",
    "    return {\"messages\": messages + [response]}\n",
    "\n",
    "\n",
    "# Define the function to execute tools\n",
    "def call_tool(state):\n",
    "    messages = state[\"messages\"]\n",
    "    # We know the last message involves at least one tool call\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    # We loop through all tool calls and append the message to our message log\n",
    "    for tool_call in last_message.additional_kwargs[\"tool_calls\"]:\n",
    "        action = ToolInvocation(\n",
    "            tool=tool_call[\"function\"][\"name\"],\n",
    "            tool_input=json.loads(tool_call[\"function\"][\"arguments\"]),\n",
    "            id=tool_call[\"id\"],\n",
    "        )\n",
    "\n",
    "        # We call the tool_executor and get back a response\n",
    "        response = tool_executor.invoke(action)\n",
    "        # We use the response to create a FunctionMessage\n",
    "        function_message = ToolMessage(\n",
    "            content=str(response), name=action.tool, tool_call_id=tool_call[\"id\"]\n",
    "        )\n",
    "\n",
    "        # Add the function message to the list\n",
    "        messages.append(function_message)\n",
    "    return {\"messages\": messages}"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# Initialize a new graph\n",
    "graph = StateGraph(AgentState)\n",
    "\n",
    "# Define the two \"Nodes\"\" we will cycle between\n",
    "graph.add_node(\"sentinel\", call_sentinel)\n",
    "graph.add_node(\"meal_planner\", call_meal_planner)\n",
    "graph.add_node(\"knowledge_master\", call_knowledge_master)\n",
    "graph.add_node(\"action\", call_tool)\n",
    "\n",
    "# Define all our Edges\n",
    "\n",
    "# Set the Starting Edge\n",
    "graph.set_entry_point(\"sentinel\")\n",
    "\n",
    "# We now add Conditional Edges\n",
    "graph.add_conditional_edges(\n",
    "    \"sentinel\",\n",
    "    lambda x: x[\"contains_information\"],\n",
    "    {\n",
    "        \"yes\": \"knowledge_master\",\n",
    "        \"no\": \"meal_planner\",\n",
    "    },\n",
    ")\n",
    "graph.add_conditional_edges(\n",
    "    \"knowledge_master\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"continue\": \"action\",\n",
    "        \"end\": END,\n",
    "    },\n",
    ")\n",
    "\n",
    "# We now add Normal Edges that should always be called after another\n",
    "graph.add_edge(\"action\", END)\n",
    "graph.add_edge(\"meal_planner\", END)\n",
    "\n",
    "# We compile the entire workflow as a runnable\n",
    "app_agent = graph.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run our Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "message = \"There are 6 people in my family. My wife doesn't eat meat and my youngest daughter is allergic to dairy.\"\n",
    "\n",
    "# message = \"Die Information, dass die Frau Fleisch isst, ist nicht länger relevant\"\n",
    "# message = \"Die Sonne scheint heute aber schön.\"\n",
    "inputs = {\n",
    "    \"messages\": [HumanMessage(content=message)],\n",
    "    \"memories\": get_knowledge()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for output in app_agent.with_config({\"run_name\": \"Memory\"}).stream(inputs):\n",
    "    # stream() yields dictionaries with output keyed by node name\n",
    "    for key, value in output.items():\n",
    "        print(f\"Output from node '{key}':\")\n",
    "        print(\"---\")\n",
    "        print(value)\n",
    "    print(\"\\n---\\n\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Installiere die Redis-Bibliothek, falls noch nicht geschehen\n",
    "# !pip install redis\n",
    "\n",
    "import redis\n",
    "\n",
    "# Verbindung zur Redis-Datenbank herstellen\n",
    "r = redis.Redis(host='localhost', port=6379, db=0, decode_responses=True)\n",
    "\n",
    "# Alle Schlüssel in der Datenbank abrufen\n",
    "keys = r.keys('*')\n",
    "# Prüfen, ob Schlüssel in der Datenbank vorhanden sind\n",
    "if keys:\n",
    "    print(\"Gefundene Schlüssel:\")\n",
    "    for key in keys:\n",
    "        # Wert für jeden Schlüssel abrufens\n",
    "        value = r.get(key)\n",
    "        print(f\"{key}: {value}\")\n",
    "else:\n",
    "    print(\"Keine Daten in der Redis-Datenbank gefunden.\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#!docker run --name redis-test -p 6379:6379 -d redis\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#!pip install fastapi langserve uvicorn sse_starlette"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
